{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework - #1\n",
    "https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/01-docker-terraform/homework.md\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Now run the command to get help on the \"docker build\" command:\n",
    "\n",
    "`docker build --help`\n",
    "\n",
    "Do the same for \"docker run\".\n",
    "Which tag has the following text? - Automatically remove the container when it exits\n",
    "\n",
    "- `--delete`\n",
    "- `--rc`\n",
    "- `--rmc`\n",
    "- **`--rm`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      --rm                             Automatically remove the container\n"
     ]
    }
   ],
   "source": [
    "!docker run --help | grep \"\\-\\-rm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1\n",
    "`--rm`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "Run docker with the python:3.9 image in an interactive mode and the entrypoint of bash. Now check the python modules that are installed ( use pip list ).\n",
    "\n",
    "What is version of the package wheel ?\n",
    "\n",
    "- 0.42.0\n",
    "- 1.0.0\n",
    "- 23.0.1\n",
    "- 58.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 0.42.0\n"
     ]
    }
   ],
   "source": [
    "!docker run --rm -it --entrypoint bash python:3.9 -c \"pip show wheel | grep Version\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2 \n",
    "\n",
    "`0.42.0`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "Run Postgres and load data as shown in the videos We'll use the green taxi trips from September 2019:\n",
    "\n",
    "https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-09.csv.gz\n",
    "\n",
    "You will also need the dataset with zones:\n",
    "\n",
    "https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
    "\n",
    "Download this data and put it into Postgres (with jupyter notebooks or with a pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_zones = \"https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\"\n",
    "url_trips = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-09.csv.gz\"\n",
    "db_user = \"root\"\n",
    "db_password = \"root\"\n",
    "db_host = \"postgres\"\n",
    "db_port = \"5432\"\n",
    "database = \"ny_taxi\"\n",
    "table_trips = \"green_tripdata\"\n",
    "table_zones = \"zone_lut\"\n",
    "parse_dates = \" \".join([\n",
    "    'tpep_pickup_datetime',\n",
    "    'tpep_dropoff_datetime',\n",
    "])\n",
    "compression = \"gzip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Running 2/0\n",
      " \u001b[32m✔\u001b[0m Container pg-database  \u001b[32mRunning\u001b[0m                                          \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pgadmin      \u001b[32mRunning\u001b[0m                                          \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 2/2\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-database  \u001b[32mRunning\u001b[0m                                          \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pgadmin      \u001b[32mRunning\u001b[0m                                          \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 2/2\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-database  \u001b[32mRunning\u001b[0m                                          \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pgadmin      \u001b[32mRunning\u001b[0m                                          \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 2/2\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-database  \u001b[32mRunning\u001b[0m                                          \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pgadmin      \u001b[32mRunning\u001b[0m                                          \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 2/2\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-database  \u001b[32mRunning\u001b[0m                                          \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pgadmin      \u001b[32mRunning\u001b[0m                                          \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l\u001b[34m[+] Running 2/2\u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pg-database  \u001b[32mHealthy\u001b[0m                                          \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container pgadmin      \u001b[32mRunning\u001b[0m                                          \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!docker compose up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Creating 1/0\n",
      " \u001b[32m✔\u001b[0m Container pg-database  \u001b[32mRunning\u001b[0m                                          \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25harguments: Namespace(user='root', password='root', host='postgres', port='5432', database='ny_taxi', table='zone_lut', url='https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv', compression=None, chunksize=100000.0, parse_dates=None)\n",
      "postgresql://root:root@postgres:5432/ny_taxi\n",
      "\n",
      "First 5 Rows:\n",
      "[]\n",
      "inserted chunk no. \"  0\" another chunk, took \"0.021\" seconds\n",
      "\n",
      "Number of lines:\n",
      "[(265,)]\n"
     ]
    }
   ],
   "source": [
    "# # ingest zones\n",
    "# !docker compose run --rm ingest_data_misc \"python ingest_data_misc/ingest_data_misc.py \\\n",
    "#     --user $db_user \\\n",
    "#     --password $db_password \\\n",
    "#     --host $db_host \\\n",
    "#     --port $db_port \\\n",
    "#     --database $database \\\n",
    "#     --table $table_zones \\\n",
    "#     --url $url_zones\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Creating 1/0\n",
      " \u001b[32m✔\u001b[0m Container pg-database  \u001b[32mRunning\u001b[0m                                          \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25harguments: Namespace(user='root', password='root', host='postgres', port='5432', database='ny_taxi', table='green_tripdata', url='https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-09.csv.gz', compression='gzip', chunksize=100000.0, parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
      "postgresql://root:root@postgres:5432/ny_taxi\n",
      "\n",
      "First 5 Rows:\n",
      "[]\n",
      "inserted chunk no. \"  0\" another chunk, took \"16.670\" seconds\n",
      "inserted chunk no. \"  1\" another chunk, took \"16.206\" seconds\n",
      "inserted chunk no. \"  2\" another chunk, took \"16.427\" seconds\n",
      "/app/ingest_data_misc/ingest_data_misc.py:31: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for i, chunk in enumerate(df_chunks):\n",
      "inserted chunk no. \"  3\" another chunk, took \"16.844\" seconds\n",
      "inserted chunk no. \"  4\" another chunk, took \"7.248\" seconds\n",
      "\n",
      "Number of lines:\n",
      "[(449063,)]\n"
     ]
    }
   ],
   "source": [
    "# # ingest trips\n",
    "# !docker compose run --rm ingest_data_misc \"python ingest_data_misc/ingest_data_misc.py \\\n",
    "#     --user $db_user \\\n",
    "#     --password $db_password \\\n",
    "#     --host $db_host \\\n",
    "#     --port $db_port \\\n",
    "#     --database $database \\\n",
    "#     --table $table_trips \\\n",
    "#     --url $url_trips \\\n",
    "#     --compression $compression \\\n",
    "#     --parse_dates $parse_dates\"\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q3. Count records\n",
    "How many taxi trips were totally made on September 18th 2019?\n",
    "\n",
    "Tip: started and finished on 2019-09-18.\n",
    "\n",
    "Remember that lpep_pickup_datetime and lpep_dropoff_datetime columns are in the format timestamp (date and hour+min+sec) and not in date.\n",
    "\n",
    "- 15767\n",
    "- **15612**\n",
    "- 15859\n",
    "- 89009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT COUNT(*) AS total_trips\n",
    "FROM \n",
    "    green_tripdata\n",
    "WHERE \n",
    "    CAST(lpep_pickup_datetime AS DATE) = '2019-09-18' AND \n",
    "    CAST(lpep_dropoff_datetime AS DATE) = '2019-09-18';\n",
    "```\n",
    "\n",
    "```text\n",
    "root@pg-database:ny_taxi> SELECT COUNT(*) AS total_trips\n",
    " FROM \n",
    "     green_tripdata\n",
    " WHERE \n",
    "     CAST(lpep_pickup_datetime AS DATE) = '2019-09-18' AND \n",
    "     CAST(lpep_dropoff_datetime AS DATE) = '2019-09-18';\n",
    "+-------------+\n",
    "| total_trips |\n",
    "|-------------|\n",
    "| 15612       |\n",
    "+-------------+\n",
    "SELECT 1\n",
    "Time: 0.363s\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3\n",
    "\n",
    "`15612`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 4. Largest trip for each day\n",
    "Which was the pick up day with the largest trip distance Use the pick up time for your calculations.\n",
    "\n",
    "- 2019-09-18\n",
    "- 2019-09-16\n",
    "- **2019-09-26**\n",
    "- 2019-09-21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT lpep_pickup_datetime, trip_distance\n",
    "FROM green_tripdata\n",
    "WHERE\n",
    "    CAST(lpep_pickup_datetime AS DATE) IN ('2019-09-18', '2019-09-16', '2019-09-26', '2019-09-21')\n",
    "ORDER BY trip_distance DESC\n",
    "LIMIT 1;\n",
    "```\n",
    "\n",
    "```text\n",
    "+----------------------+---------------+\n",
    "| lpep_pickup_datetime | trip_distance |\n",
    "|----------------------+---------------|\n",
    "| 2019-09-26 19:32:52  | 341.64        |\n",
    "+----------------------+---------------+\n",
    "SELECT 1\n",
    "Time: 0.360s\n",
    "```\n",
    "\n",
    "```sql\t\n",
    "SELECT \n",
    "    CAST(lpep_pickup_datetime AS DATE) as \"day\", \n",
    "    trip_distance as max_distance\n",
    "FROM \n",
    "    green_tripdata\n",
    "WHERE\n",
    "    CAST(lpep_pickup_datetime AS DATE) IN ('2019-09-18', '2019-09-16', '2019-09-26', '2019-09-21') AND \n",
    "    trip_distance = (\n",
    "        SELECT MAX(trip_distance)\n",
    "        FROM green_tripdata\n",
    "        WHERE CAST(lpep_pickup_datetime AS DATE) IN ('2019-09-18', '2019-09-16', '2019-09-26', '2019-09-21')\n",
    "    );\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A4\n",
    "\n",
    "`2019-09-26`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Three biggest pick up Boroughs\n",
    "Consider lpep_pickup_datetime in '2019-09-18' and ignoring Borough has Unknown\n",
    "\n",
    "Which were the 3 pick up Boroughs that had a sum of total_amount superior to 50000?\n",
    "\n",
    "- **\"Brooklyn\" \"Manhattan\" \"Queens\"**\n",
    "- \"Bronx\" \"Brooklyn\" \"Manhattan\"\n",
    "- \"Bronx\" \"Manhattan\" \"Queens\"\n",
    "- \"Brooklyn\" \"Queens\" \"Staten Island\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT\n",
    "    CAST(t.\"lpep_pickup_datetime\" AS DATE) as \"day\",\n",
    "    z.\"Borough\",\n",
    "    SUM(t.\"total_amount\") AS total_amount\n",
    "FROM\n",
    "    green_tripdata t\n",
    "    LEFT JOIN zone_lut z ON t.\"PULocationID\" = z.\"LocationID\"\n",
    "WHERE\n",
    "    CAST(t.\"lpep_pickup_datetime\" AS DATE) = '2019-09-18' AND\n",
    "    z.\"Borough\" != 'Unknown'\n",
    "GROUP BY\n",
    "    z.\"Borough\", CAST(t.\"lpep_pickup_datetime\" AS DATE)\n",
    "HAVING\n",
    "    SUM(t.\"total_amount\") > 50000\n",
    "ORDER BY\n",
    "    total_amount DESC;\n",
    "```\n",
    "```text\n",
    "+------------+-----------+-------------------+\n",
    "| day        | Borough   | total_amount      |\n",
    "|------------+-----------+-------------------|\n",
    "| 2019-09-18 | Brooklyn  | 96333.23999999976 |\n",
    "| 2019-09-18 | Manhattan | 92271.29999999913 |\n",
    "| 2019-09-18 | Queens    | 78671.70999999877 |\n",
    "+------------+-----------+-------------------+\n",
    "```\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM (\n",
    "    SELECT\n",
    "        CAST(t.\"lpep_pickup_datetime\" AS DATE) as \"day\",\n",
    "        z.\"Borough\",\n",
    "        SUM(t.\"total_amount\") AS total_amount\n",
    "    FROM\n",
    "        green_tripdata t\n",
    "        LEFT JOIN zone_lut z ON t.\"PULocationID\" = z.\"LocationID\"\n",
    "    WHERE\n",
    "        CAST(t.\"lpep_pickup_datetime\" AS DATE) = '2019-09-18' AND\n",
    "        z.\"Borough\" != 'Unknown'\n",
    "    GROUP BY\n",
    "        z.\"Borough\", CAST(t.\"lpep_pickup_datetime\" AS DATE)\n",
    ") AS subquery\n",
    "WHERE\n",
    "    total_amount > 50000\n",
    "ORDER BY\n",
    "    total_amount DESC;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A5\n",
    "\n",
    "**\"Brooklyn\" \"Manhattan\" \"Queens\"**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 6. Largest tip\n",
    "For the passengers picked up in September 2019 in the zone name Astoria which was the drop off zone that had the largest tip? We want the name of the zone, not the id.\n",
    "\n",
    "Note: it's not a typo, it's tip , not trip\n",
    "\n",
    "- Central Park\n",
    "- Jamaica\n",
    "- JFK Airport\n",
    "- Long Island City/Queens Plaza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT\n",
    "    CAST(t.\"lpep_pickup_datetime\" AS DATE) as \"day\",\n",
    "    zpu.\"Zone\" as \"pickup_zone\",\n",
    "    zdo.\"Zone\" as \"dropoff_zone\",\n",
    "    t.\"tip_amount\"\n",
    "FROM\n",
    "    green_tripdata t\n",
    "    JOIN zone_lut zpu ON t.\"PULocationID\" = zpu.\"LocationID\"\n",
    "    JOIN zone_lut zdo ON t.\"DOLocationID\" = zdo.\"LocationID\"\n",
    "WHERE\n",
    "    zpu.\"Zone\" = 'Astoria'\n",
    "ORDER BY\n",
    "    t.\"tip_amount\" DESC\n",
    "LIMIT 1;\n",
    "```\n",
    "\n",
    "```text\n",
    "+------------+-------------+--------------+------------+\n",
    "| day        | pickup_zone | dropoff_zone | tip_amount |\n",
    "|------------+-------------+--------------+------------|\n",
    "| 2019-09-08 | Astoria     | JFK Airport  | 62.31      |\n",
    "+------------+-------------+--------------+------------+\n",
    "SELECT 1\n",
    "Time: 0.094s\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A6\n",
    "\n",
    "`JFK Airport`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terraform\n",
    "In this section homework we'll prepare the environment by creating resources in GCP with Terraform.\n",
    "\n",
    "In your VM on GCP/Laptop/GitHub Codespace install Terraform. Copy the files from the course repo here to your VM/Laptop/GitHub Codespace.\n",
    "\n",
    "Modify the files as necessary to create a GCP Bucket and Big Query Dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Q 7. Creating Resources\n",
    "After updating the main.tf and variable.tf files run:\n",
    "\n",
    "terraform apply\n",
    "Paste the output of this command into the homework submission form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sy23/github/dezoomcamp/homework/01-docker-terraform'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "cd homework/01-docker-terraform\n",
    "docker compose run --rm gcloud-config\n",
    "\n",
    "docker compose run --rm terraform init\n",
    "docker compose run --rm terraform plant\n",
    "docker compose run --rm terraform apply\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Terraform used the selected providers to generate the following execution\n",
    "plan. Resource actions are indicated with the following symbols:\n",
    "  + create\n",
    "\n",
    "Terraform will perform the following actions:\n",
    "\n",
    "  # google_bigquery_dataset.demo_dataset will be created\n",
    "  + resource \"google_bigquery_dataset\" \"demo_dataset\" {\n",
    "      + creation_time              = (known after apply)\n",
    "      + dataset_id                 = \"demo_dataset\"\n",
    "      + default_collation          = (known after apply)\n",
    "      + delete_contents_on_destroy = false\n",
    "      + effective_labels           = (known after apply)\n",
    "      + etag                       = (known after apply)\n",
    "      + id                         = (known after apply)\n",
    "      + is_case_insensitive        = (known after apply)\n",
    "      + last_modified_time         = (known after apply)\n",
    "      + location                   = \"US\"\n",
    "      + max_time_travel_hours      = (known after apply)\n",
    "      + project                    = \"some-project-name-123\"\n",
    "      + self_link                  = (known after apply)\n",
    "      + storage_billing_model      = (known after apply)\n",
    "      + terraform_labels           = (known after apply)\n",
    "    }\n",
    "\n",
    "  # google_storage_bucket.demo-bucket will be created\n",
    "  + resource \"google_storage_bucket\" \"demo-bucket\" {\n",
    "      + effective_labels            = (known after apply)\n",
    "      + force_destroy               = true\n",
    "      + id                          = (known after apply)\n",
    "      + location                    = \"US\"\n",
    "      + name                        = \"terraform-demo-terra-bucket\"\n",
    "      + project                     = (known after apply)\n",
    "      + public_access_prevention    = (known after apply)\n",
    "      + self_link                   = (known after apply)\n",
    "      + storage_class               = \"STANDARD\"\n",
    "      + terraform_labels            = (known after apply)\n",
    "      + uniform_bucket_level_access = (known after apply)\n",
    "      + url                         = (known after apply)\n",
    "\n",
    "      + lifecycle_rule {\n",
    "          + action {\n",
    "              + type = \"AbortIncompleteMultipartUpload\"\n",
    "            }\n",
    "          + condition {\n",
    "              + age                   = 1\n",
    "              + matches_prefix        = []\n",
    "              + matches_storage_class = []\n",
    "              + matches_suffix        = []\n",
    "              + with_state            = (known after apply)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "Plan: 2 to add, 0 to change, 0 to destroy.\n",
    "\n",
    "Do you want to perform these actions?\n",
    "  Terraform will perform the actions described above.\n",
    "  Only 'yes' will be accepted to approve.\n",
    "\n",
    "  Enter a value: \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitting the solutions\n",
    "Form for submitting: https://courses.datatalks.club/de-zoomcamp-2024/homework/hw01\n",
    "You can submit your homework multiple times. In this case, only the last submission will be used.\n",
    "Deadline: 29 January, 23:00 CET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
